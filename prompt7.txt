import os
import re
import time
import hashlib
import logging
from typing import List, Dict, Any, Tuple, Optional
from pydantic import BaseModel
import fitz  # PyMuPDF

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Azure Document Intelligence client (configure with your credentials)
# from azure.ai.formrecognizer import DocumentAnalysisClient
# from azure.core.credentials import AzureKeyCredential
# document_client = DocumentAnalysisClient(
#     endpoint="YOUR_ENDPOINT",
#     credential=AzureKeyCredential("YOUR_KEY")
# )

# Pydantic Models
class PODocumentInfo(BaseModel):
    filename: str
    po_number: Optional[str]
    extracted_data: Dict[str, Any]
    page_count: int
    processed_at: float
    file_hash: str
    sequence_number: int

class ProcessingStatus(BaseModel):
    total_files: int
    processed_files: int
    pending_files: int
    recently_processed: List[str]
    error_files: List[str]
    is_running: bool


def calculate_file_hash(file_bytes: bytes) -> str:
    """Calculate SHA256 hash of file content."""
    return hashlib.sha256(file_bytes).hexdigest()


def split_pdf_pages(pdf_bytes: bytes) -> Optional[List[bytes]]:
    """
    Split a multi-page PDF into individual single-page PDFs.
    
    Args:
        pdf_bytes: Raw bytes of the PDF file
        
    Returns:
        List of single-page PDF bytes, or None if error
    """
    try:
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        logger.info(f"Opening PDF with {doc.page_count} pages (size={len(pdf_bytes)} bytes)")
    except Exception as e:
        logger.error(f"split_pdf_pages: Couldn't open PDF: {e}")
        return None

    page_byte_list = []
    for page_num in range(doc.page_count):
        single_pdf = fitz.open()
        try:
            single_pdf.insert_pdf(doc, from_page=page_num, to_page=page_num)
            if single_pdf.page_count != 1:
                logger.error(f"Failed to extract page {page_num} (got {single_pdf.page_count} pages)")
                continue
            buf = single_pdf.write()
            page_byte_list.append(bytes(buf))
            logger.info(f"Extracted page {page_num + 1}/{doc.page_count}")
        except Exception as e:
            logger.error(f"split_pdf_pages: Couldn't extract page {page_num}: {e}")
        finally:
            single_pdf.close()
    
    doc.close()
    logger.info(f"Successfully split PDF into {len(page_byte_list)} pages")
    return page_byte_list


def extract_po_number(page_bytes: bytes) -> Optional[str]:
    """
    Extract PO number from a PDF page using Azure Document Intelligence 
    and regex fallback.
    
    Args:
        page_bytes: Single-page PDF as bytes
        
    Returns:
        PO number as string, or None if not found
    """
    # Method 1: Azure Document Intelligence (uncomment when configured)
    # try:
    #     poller = document_client.begin_analyze_document("prebuilt-document", page_bytes)
    #     result = poller.result()
    #     
    #     # Search for common PO number field names
    #     for doc in result.documents:
    #         for key in ["PurchaseOrder", "PONumber", "PO_Number", "PO#", "Purchase Order", "OrderNumber"]:
    #             val = doc.fields.get(key)
    #             if val and getattr(val, "value", None):
    #                 logger.info(f"Found PO number via Document Intelligence: {val.value}")
    #                 return str(val.value)
    # except Exception as e:
    #     logger.warning(f"Document Intelligence extraction failed: {str(e)}")
    
    # Method 2: Text extraction with regex fallback
    try:
        doc = fitz.open(stream=page_bytes, filetype="pdf")
        
        if doc.page_count < 1:
            logger.error(f"PDF has zero pages! Cannot extract text.")
            return None
            
        text = doc[0].get_text("text")
        doc.close()
        
        # Common PO number patterns
        patterns = [
            r'PO\s*#[:\s]*([A-Za-z0-9\-\/]+)',
            r'P\.?O\.?\s*Number[:\s]*([A-Za-z0-9\-\/]+)',
            r'Purchase\s*Order\s*#?[:\s]*([A-Za-z0-9\-\/]+)',
            r'Order\s*Number[:\s]*([A-Za-z0-9\-\/]+)',
            r'PO[:\s]+([A-Za-z0-9\-\/]{5,})',  # Generic PO followed by alphanumeric
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                po_number = match.group(1).strip()
                logger.info(f"Found PO number via regex: {po_number}")
                return po_number
                
    except Exception as e:
        logger.error(f"Text extraction error: {str(e)}")
    
    return None


def group_pages_by_po(page_bytes_list: List[bytes]) -> List[Dict[str, Any]]:
    """
    Group pages by PO number. Pages with the same PO number are grouped together.
    
    Args:
        page_bytes_list: List of single-page PDF bytes
        
    Returns:
        List of dictionaries with 'po_number' and 'pages' keys
    """
    groups = []
    current_group = []
    current_po_number = None

    for i, page_bytes in enumerate(page_bytes_list):
        logger.info(f"Processing page {i + 1}/{len(page_bytes_list)}")
        po_num = extract_po_number(page_bytes)
        
        if po_num:
            # Found a PO number on this page
            if current_po_number is None:
                # Start first group
                current_group = [page_bytes]
                current_po_number = po_num
                logger.info(f"Starting new PO group: {po_num}")
            elif po_num == current_po_number:
                # Same PO, add to current group
                current_group.append(page_bytes)
                logger.info(f"Added page to PO {po_num} (now {len(current_group)} pages)")
            else:
                # Different PO, save current group and start new one
                logger.info(f"Completing PO {current_po_number} with {len(current_group)} pages")
                groups.append({
                    'po_number': current_po_number, 
                    'pages': current_group
                })
                current_group = [page_bytes]
                current_po_number = po_num
                logger.info(f"Starting new PO group: {po_num}")
        else:
            # No PO number found, add to current group
            if not current_group:
                # Start a group without PO number
                current_group = [page_bytes]
                current_po_number = None
                logger.warning(f"Page {i + 1}: No PO number found, starting unnamed group")
            else:
                # Add to existing group
                current_group.append(page_bytes)
                logger.info(f"Page {i + 1}: No PO number, added to current group")
    
    # Add the last group
    if current_group:
        logger.info(f"Completing final PO {current_po_number} with {len(current_group)} pages")
        groups.append({
            'po_number': current_po_number, 
            'pages': current_group
        })
    
    logger.info(f"Total PO groups created: {len(groups)}")
    return groups


def assemble_pdf_from_pages(page_bytes_group: List[bytes]) -> bytes:
    """
    Combine multiple single-page PDFs into one multi-page PDF.
    
    Args:
        page_bytes_group: List of single-page PDF bytes
        
    Returns:
        Combined PDF as bytes
    """
    pdf = fitz.open()
    
    for idx, page_bytes in enumerate(page_bytes_group):
        try:
            single_doc = fitz.open(stream=page_bytes, filetype='pdf')
            pdf.insert_pdf(single_doc, from_page=0, to_page=0)
            single_doc.close()
        except Exception as e:
            logger.error(f"Error assembling page {idx + 1}: {e}")
    
    out_bytes = pdf.write()
    pdf.close()
    
    return bytes(out_bytes)


def extract_po_data(pdf_bytes: bytes, filename: str) -> Dict[str, Any]:
    """
    Extract structured data from PO using Azure Document Intelligence.
    
    Args:
        pdf_bytes: PDF file as bytes
        filename: Original filename
        
    Returns:
        Dictionary containing extracted PO data
    """
    extracted_data = {
        "filename": filename,
        "extraction_method": "regex_fallback",
        "fields": {}
    }
    
    # Uncomment when Azure Document Intelligence is configured
    # try:
    #     poller = document_client.begin_analyze_document("prebuilt-document", pdf_bytes)
    #     result = poller.result()
    #     
    #     extracted_data["extraction_method"] = "azure_document_intelligence"
    #     
    #     for doc in result.documents:
    #         for field_name, field_value in doc.fields.items():
    #             if field_value and hasattr(field_value, 'value'):
    #                 extracted_data["fields"][field_name] = str(field_value.value)
    #     
    #     logger.info(f"Extracted {len(extracted_data['fields'])} fields via Document Intelligence")
    #     
    # except Exception as e:
    #     logger.error(f"Document Intelligence extraction failed: {e}")
    
    return extracted_data


def save_pdf_to_file(pdf_bytes: bytes, output_path: str) -> bool:
    """
    Save PDF bytes to a file.
    
    Args:
        pdf_bytes: PDF content as bytes
        output_path: Full path where to save the file
        
    Returns:
        True if successful, False otherwise
    """
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'wb') as f:
            f.write(pdf_bytes)
        logger.info(f"Saved PDF to: {output_path}")
        return True
    except Exception as e:
        logger.error(f"Error saving PDF to {output_path}: {e}")
        return False


def extract_pos_from_pdf(pdf_bytes: bytes, original_filename: str, 
                         output_dir: str = "./output") -> List[PODocumentInfo]:
    """
    Main function to extract and process multiple POs from a single PDF.
    
    Args:
        pdf_bytes: Raw bytes of the input PDF
        original_filename: Original filename for reference
        output_dir: Directory where to save split PO files
        
    Returns:
        List of PODocumentInfo objects for each extracted PO
    """
    logger.info(f"Starting PO extraction from: {original_filename}")
    
    # Step 1: Split PDF into individual pages
    page_bytes_list = split_pdf_pages(pdf_bytes)
    if not page_bytes_list:
        logger.error("Failed to split PDF into pages")
        return []
    
    # Step 2: Group pages by PO number
    groups = group_pages_by_po(page_bytes_list)
    
    # Step 3: Process each group
    po_documents = []
    base_filename = os.path.splitext(original_filename)[0]
    
    for idx, group in enumerate(groups, 1):
        if not group['pages']:
            logger.warning(f"Group {idx}: No pages to process, skipping")
            continue
        
        po_number = group['po_number'] or f"UNKNOWN_{idx}"
        logger.info(f"Processing PO {idx}/{len(groups)}: {po_number} ({len(group['pages'])} pages)")
        
        # Validate pages
        valid_group = True
        for page_idx, page_bytes in enumerate(group['pages']):
            try:
                doc = fitz.open(stream=page_bytes, filetype='pdf')
                if doc.page_count != 1:
                    logger.error(f"Page {page_idx + 1} is not a valid single-page PDF")
                    valid_group = False
                doc.close()
            except Exception as e:
                logger.error(f"Could not validate page {page_idx + 1}: {e}")
                valid_group = False
        
        if not valid_group:
            logger.warning(f"Skipping invalid group for PO {po_number}")
            continue
        
        # Assemble multi-page PO PDF
        po_pdf_bytes = assemble_pdf_from_pages(group['pages'])
        
        # Generate filename
        po_filename = f"{base_filename}_PO_{po_number}.pdf"
        po_filepath = os.path.join(output_dir, po_filename)
        
        # Save to file
        save_pdf_to_file(po_pdf_bytes, po_filepath)
        
        # Extract data
        extracted_data = extract_po_data(po_pdf_bytes, po_filename)
        
        # Create document info
        doc_info = PODocumentInfo(
            filename=po_filename,
            po_number=po_number,
            extracted_data=extracted_data,
            page_count=len(group['pages']),
            processed_at=time.time(),
            file_hash=calculate_file_hash(po_pdf_bytes),
            sequence_number=idx
        )
        
        po_documents.append(doc_info)
        logger.info(f"Successfully processed PO: {po_filename}")
    
    logger.info(f"Extraction complete. Processed {len(po_documents)} POs from {original_filename}")
    return po_documents


def main():
    """Example usage of the PO extraction system."""
    
    # Example: Process a PDF file
    input_pdf_path = "sample_multi_po.pdf"
    output_directory = "./extracted_pos"
    
    try:
        # Read the input PDF
        with open(input_pdf_path, 'rb') as f:
            pdf_bytes = f.read()
        
        # Extract and process POs
        po_documents = extract_pos_from_pdf(
            pdf_bytes=pdf_bytes,
            original_filename=os.path.basename(input_pdf_path),
            output_dir=output_directory
        )
        
        # Print summary
        print("\n" + "="*60)
        print("EXTRACTION SUMMARY")
        print("="*60)
        for doc in po_documents:
            print(f"\nPO Number: {doc.po_number}")
            print(f"Filename: {doc.filename}")
            print(f"Pages: {doc.page_count}")
            print(f"File Hash: {doc.file_hash[:16]}...")
            print(f"Processed: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(doc.processed_at))}")
        
        print("\n" + "="*60)
        print(f"Total POs extracted: {len(po_documents)}")
        print("="*60)
        
    except FileNotFoundError:
        logger.error(f"Input file not found: {input_pdf_path}")
    except Exception as e:
        logger.error(f"Error during processing: {e}")


if __name__ == "__main__":
    main()
