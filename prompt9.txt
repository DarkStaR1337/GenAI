import os
import re
import time
import hashlib
import logging
from typing import List, Dict, Any, Tuple, Optional
from pydantic import BaseModel
import fitz  # PyMuPDF
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Azure Document Intelligence Configuration
AZURE_ENDPOINT = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT", "YOUR_ENDPOINT_HERE")
AZURE_KEY = os.getenv("AZURE_DOCUMENT_INTELLIGENCE_KEY", "YOUR_KEY_HERE")

# Initialize Azure Document Intelligence Client
document_client = DocumentAnalysisClient(
    endpoint=AZURE_ENDPOINT,
    credential=AzureKeyCredential(AZURE_KEY)
)

# Pydantic Models
class PODocumentInfo(BaseModel):
    filename: str
    po_number: Optional[str]
    page_number_info: Optional[str]
    extracted_data: Dict[str, Any]
    page_count: int
    original_page_range: str  # e.g., "1-4" or "5-8"
    processed_at: float
    file_hash: str
    sequence_number: int
    grouping_method: str

class ProcessingStatus(BaseModel):
    total_files: int
    processed_files: int
    pending_files: int
    recently_processed: List[str]
    error_files: List[str]
    is_running: bool


def calculate_file_hash(file_bytes: bytes) -> str:
    """Calculate SHA256 hash of file content."""
    return hashlib.sha256(file_bytes).hexdigest()


def split_pdf_pages(pdf_bytes: bytes) -> Optional[List[bytes]]:
    """
    Split a multi-page PDF into individual single-page PDFs.
    
    Args:
        pdf_bytes: Raw bytes of the PDF file
        
    Returns:
        List of single-page PDF bytes, or None if error
    """
    try:
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
        total_pages = doc.page_count
        logger.info(f"Opening PDF with {total_pages} pages (size={len(pdf_bytes)} bytes)")
        
        if total_pages == 0:
            logger.error("PDF has 0 pages")
            doc.close()
            return None
            
    except Exception as e:
        logger.error(f"split_pdf_pages: Couldn't open PDF: {e}")
        return None

    page_byte_list = []
    for page_num in range(total_pages):
        single_pdf = fitz.open()
        try:
            # Insert only this specific page
            single_pdf.insert_pdf(doc, from_page=page_num, to_page=page_num)
            
            if single_pdf.page_count != 1:
                logger.error(f"Failed to extract page {page_num + 1} (got {single_pdf.page_count} pages)")
                single_pdf.close()
                continue
            
            # Write to bytes
            buf = single_pdf.write()
            page_byte_list.append(bytes(buf))
            logger.info(f"✓ Extracted page {page_num + 1}/{total_pages}")
            
        except Exception as e:
            logger.error(f"split_pdf_pages: Couldn't extract page {page_num + 1}: {e}")
        finally:
            single_pdf.close()
    
    doc.close()
    logger.info(f"Successfully split PDF into {len(page_byte_list)} pages")
    return page_byte_list if page_byte_list else None


def extract_page_number_from_text(page_bytes: bytes) -> Optional[Tuple[int, Optional[int]]]:
    """
    Extract page number from PDF text content.
    Looks for patterns like "Page 1 of 5", "1/5", etc.
    
    Args:
        page_bytes: Single-page PDF as bytes
        
    Returns:
        Tuple of (current_page, total_pages) or None if not found
    """
    try:
        doc = fitz.open(stream=page_bytes, filetype="pdf")
        
        if doc.page_count < 1:
            doc.close()
            return None
            
        text = doc[0].get_text("text")
        doc.close()
        
        # Page number patterns
        patterns = [
            (r'Page\s+(\d+)\s+of\s+(\d+)', True),   # "Page 1 of 5" - has total
            (r'Page\s+(\d+)/(\d+)', True),          # "Page 1/5" - has total
            (r'(\d+)\s+of\s+(\d+)', True),          # "1 of 5" - has total
            (r'(\d+)/(\d+)', True),                 # "1/5" - has total
            (r'Page\s+(\d+)', False),               # "Page 1" - no total
        ]
        
        for pattern, has_total in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                if has_total:
                    current_page = int(match.group(1))
                    total_pages = int(match.group(2))
                    logger.debug(f"Found page number (text): {current_page}/{total_pages}")
                    return (current_page, total_pages)
                else:
                    current_page = int(match.group(1))
                    logger.debug(f"Found page number (text): Page {current_page}")
                    return (current_page, None)
                    
    except Exception as e:
        logger.error(f"Text extraction error for page number: {str(e)}")
    
    return None


class PageAnalysisCache:
    """Cache for Document Intelligence analysis results to avoid duplicate API calls."""
    
    def __init__(self):
        self.cache = {}  # Key: page hash, Value: DI result
    
    def get_page_hash(self, page_bytes: bytes) -> str:
        """Generate a hash for page bytes to use as cache key."""
        return hashlib.md5(page_bytes).hexdigest()
    
    def get(self, page_bytes: bytes):
        """Get cached analysis result for a page."""
        page_hash = self.get_page_hash(page_bytes)
        return self.cache.get(page_hash)
    
    def set(self, page_bytes: bytes, result):
        """Cache analysis result for a page."""
        page_hash = self.get_page_hash(page_bytes)
        self.cache[page_hash] = result
    
    def clear(self):
        """Clear the cache."""
        self.cache.clear()


# Global cache instance
di_cache = PageAnalysisCache()


def analyze_page_with_di(page_bytes: bytes) -> Optional[Any]:
    """
    Analyze page with Document Intelligence once and cache the result.
    
    Args:
        page_bytes: Single-page PDF as bytes
        
    Returns:
        Document Intelligence result object or None if failed
    """
    # Check cache first
    cached_result = di_cache.get(page_bytes)
    if cached_result is not None:
        logger.debug("Using cached DI result")
        return cached_result
    
    # Not in cache, call DI API
    try:
        logger.debug("Calling Document Intelligence API...")
        poller = document_client.begin_analyze_document(
            model_id="prebuilt-document",
            document=page_bytes
        )
        result = poller.result()
        
        # Cache the result
        di_cache.set(page_bytes, result)
        logger.debug("Cached DI result")
        
        return result
        
    except Exception as e:
        logger.error(f"Document Intelligence API error: {str(e)}")
        # Cache None to avoid retrying failed pages
        di_cache.set(page_bytes, None)
        return None


def extract_page_number_from_di_result(di_result) -> Optional[Tuple[int, Optional[int]]]:
    """
    Extract page number from cached DI result.
    
    Args:
        di_result: Document Intelligence result object
        
    Returns:
        Tuple of (current_page, total_pages) or None if not found
    """
    if not di_result:
        return None
    
    try:
        full_text = ""
        if di_result.content:
            full_text = di_result.content
        
        if full_text:
            patterns = [
                (r'Page\s+(\d+)\s+of\s+(\d+)', True),
                (r'Page\s+(\d+)/(\d+)', True),
                (r'(\d+)\s+of\s+(\d+)', True),
                (r'(\d+)/(\d+)', True),
                (r'Page\s+(\d+)', False),
            ]
            
            for pattern, has_total in patterns:
                match = re.search(pattern, full_text, re.IGNORECASE)
                if match:
                    if has_total:
                        current_page = int(match.group(1))
                        total_pages = int(match.group(2))
                        logger.debug(f"Found page number (DI): {current_page}/{total_pages}")
                        return (current_page, total_pages)
                    else:
                        current_page = int(match.group(1))
                        logger.debug(f"Found page number (DI): Page {current_page}")
                        return (current_page, None)
        
    except Exception as e:
        logger.error(f"Error extracting page number from DI result: {str(e)}")
    
    return None


def extract_po_number_from_di_result(di_result) -> Optional[str]:
    """
    Extract PO number from cached DI result.
    
    Args:
        di_result: Document Intelligence result object
        
    Returns:
        PO number as string, or None if not found
    """
    if not di_result:
        return None
    
    try:
        full_text = ""
        if di_result.content:
            full_text = di_result.content
        
        # Check key-value pairs
        if di_result.key_value_pairs:
            for kv in di_result.key_value_pairs:
                if kv.key and kv.value:
                    key_text = kv.key.content.lower() if kv.key.content else ""
                    value_text = kv.value.content if kv.value.content else ""
                    
                    po_keywords = ["po", "purchase order", "order number", "po number", "p.o", "order no"]
                    if any(keyword in key_text for keyword in po_keywords):
                        logger.debug(f"Found PO via key-value pair: {value_text}")
                        return value_text.strip()
        
        # Search in full content with regex
        if full_text:
            patterns = [
                r'ORDER\s+NO\.?\s*[:\s]*([A-Za-z0-9\-\/]+)',
                r'NO\.\s*[:\s]*([A-Za-z0-9\-\/]+)',
                r'PO\s*#[:\s]*([A-Za-z0-9\-\/]+)',
                r'P\.?O\.?\s*Number[:\s]*([A-Za-z0-9\-\/]+)',
                r'Purchase\s*Order\s*#?[:\s]*([A-Za-z0-9\-\/]+)',
                r'Order\s*Number[:\s]*([A-Za-z0-9\-\/]+)',
                r'PO[:\s]+([A-Za-z0-9\-\/]{5,})',
            ]
            
            for pattern in patterns:
                match = re.search(pattern, full_text, re.IGNORECASE)
                if match:
                    po_number = match.group(1).strip()
                    logger.debug(f"Found PO via DI content + regex: {po_number}")
                    return po_number
        
    except Exception as e:
        logger.error(f"Error extracting PO number from DI result: {str(e)}")
    
    return None


def extract_po_number_fallback(page_bytes: bytes) -> Optional[str]:
    """
    Fallback method: Extract PO number using PyMuPDF text extraction and regex.
    
    Args:
        page_bytes: Single-page PDF as bytes
        
    Returns:
        PO number as string, or None if not found
    """
    try:
        doc = fitz.open(stream=page_bytes, filetype="pdf")
        
        if doc.page_count < 1:
            doc.close()
            return None
            
        text = doc[0].get_text("text")
        doc.close()
        
        patterns = [
            r'ORDER\s+NO\.?\s*[:\s]*([A-Za-z0-9\-\/]+)',
            r'NO\.\s*[:\s]*([A-Za-z0-9\-\/]+)',
            r'PO\s*#[:\s]*([A-Za-z0-9\-\/]+)',
            r'P\.?O\.?\s*Number[:\s]*([A-Za-z0-9\-\/]+)',
            r'Purchase\s*Order\s*#?[:\s]*([A-Za-z0-9\-\/]+)',
            r'Order\s*Number[:\s]*([A-Za-z0-9\-\/]+)',
            r'PO[:\s]+([A-Za-z0-9\-\/]{5,})',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                po_number = match.group(1).strip()
                logger.debug(f"Found PO via fallback regex: {po_number}")
                return po_number
                
    except Exception as e:
        logger.error(f"Fallback text extraction error: {str(e)}")
    
    return None


def detect_document_boundaries(page_bytes_list: List[bytes], use_di: bool = True) -> List[int]:
    """
    Detect where new documents start in the page list.
    Returns list of page indices where new documents begin.
    
    Priority:
    1. When page number = 1 (from text or DI)
    2. When PO number changes (from DI or regex)
    
    Args:
        page_bytes_list: List of single-page PDF bytes
        use_di: Whether to use Document Intelligence
        
    Returns:
        List of indices where new documents start (always includes 0)
    """
    boundaries = [0]  # First page always starts a document
    last_po_number = None
    
    logger.info(f"\n{'='*60}")
    logger.info(f"Detecting document boundaries in {len(page_bytes_list)} pages")
    logger.info(f"Using Document Intelligence: {use_di}")
    logger.info(f"{'='*60}\n")
    
    for i, page_bytes in enumerate(page_bytes_list):
        logger.info(f"Analyzing page {i + 1}/{len(page_bytes_list)}...")
        
        # Get DI result once (cached if already analyzed)
        di_result = None
        if use_di:
            di_result = analyze_page_with_di(page_bytes)
        
        # Priority 1: Check for page number = 1 (text method first - fastest)
        page_info = extract_page_number_from_text(page_bytes)
        if page_info and page_info[0] == 1 and i > 0:
            logger.info(f"  ✓ Found 'Page 1' at position {i + 1} (text) - NEW DOCUMENT")
            boundaries.append(i)
            last_po_number = None  # Reset PO tracking
            continue
        
        # Priority 2: Check for page number = 1 (DI method - use cached result)
        if di_result:
            page_info = extract_page_number_from_di_result(di_result)
            if page_info and page_info[0] == 1 and i > 0:
                logger.info(f"  ✓ Found 'Page 1' at position {i + 1} (DI) - NEW DOCUMENT")
                boundaries.append(i)
                last_po_number = None
                continue
        
        # Priority 3: Check for PO number change (DI method - use cached result)
        if di_result:
            po_number = extract_po_number_from_di_result(di_result)
            if po_number:
                if last_po_number is None:
                    last_po_number = po_number
                    logger.info(f"  → PO number: {po_number}")
                elif po_number != last_po_number and i > 0:
                    logger.info(f"  ✓ PO changed from '{last_po_number}' to '{po_number}' at position {i + 1} (DI) - NEW DOCUMENT")
                    boundaries.append(i)
                    last_po_number = po_number
                    continue
                else:
                    logger.info(f"  → PO number: {po_number} (same)")
        
        # Priority 4: Check for PO number change (regex fallback)
        if not di_result or last_po_number is None:
            po_number = extract_po_number_fallback(page_bytes)
            if po_number:
                if last_po_number is None:
                    last_po_number = po_number
                    logger.info(f"  → PO number: {po_number} (regex)")
                elif po_number != last_po_number and i > 0:
                    logger.info(f"  ✓ PO changed from '{last_po_number}' to '{po_number}' at position {i + 1} (regex) - NEW DOCUMENT")
                    boundaries.append(i)
                    last_po_number = po_number
                    continue
                else:
                    logger.info(f"  → PO number: {po_number} (same, regex)")
    
    logger.info(f"\n{'='*60}")
    logger.info(f"Detected {len(boundaries)} document(s)")
    for idx, boundary in enumerate(boundaries):
        end_idx = boundaries[idx + 1] - 1 if idx + 1 < len(boundaries) else len(page_bytes_list) - 1
        logger.info(f"  Document {idx + 1}: Pages {boundary + 1} to {end_idx + 1}")
    logger.info(f"{'='*60}\n")
    
    return boundaries


def group_pages_by_boundaries(page_bytes_list: List[bytes], boundaries: List[int]) -> List[Dict[str, Any]]:
    """
    Group pages based on detected boundaries.
    
    Args:
        page_bytes_list: List of single-page PDF bytes
        boundaries: List of indices where documents start
        
    Returns:
        List of dictionaries with page ranges and pages
    """
    groups = []
    
    for i in range(len(boundaries)):
        start_idx = boundaries[i]
        end_idx = boundaries[i + 1] if i + 1 < len(boundaries) else len(page_bytes_list)
        
        pages = page_bytes_list[start_idx:end_idx]
        page_range = f"{start_idx + 1}-{end_idx}"
        
        groups.append({
            'start_page': start_idx + 1,
            'end_page': end_idx,
            'page_range': page_range,
            'pages': pages
        })
        
        logger.info(f"Group {i + 1}: Original pages {page_range} ({len(pages)} pages)")
    
    return groups


def assemble_pdf_from_pages(page_bytes_group: List[bytes]) -> bytes:
    """
    Combine multiple single-page PDFs into one multi-page PDF.
    Pages will be renumbered starting from 1.
    
    Args:
        page_bytes_group: List of single-page PDF bytes
        
    Returns:
        Combined PDF as bytes
    """
    pdf = fitz.open()
    
    for idx, page_bytes in enumerate(page_bytes_group):
        try:
            single_doc = fitz.open(stream=page_bytes, filetype='pdf')
            pdf.insert_pdf(single_doc, from_page=0, to_page=0)
            single_doc.close()
            logger.debug(f"Assembled page {idx + 1}/{len(page_bytes_group)}")
        except Exception as e:
            logger.error(f"Error assembling page {idx + 1}: {e}")
    
    out_bytes = pdf.write()
    pdf.close()
    
    logger.info(f"Successfully assembled {len(page_bytes_group)} pages into PDF (renumbered 1-{len(page_bytes_group)})")
    return bytes(out_bytes)


def extract_po_data_with_di(pdf_bytes: bytes, filename: str) -> Dict[str, Any]:
    """
    Extract structured data from PO using Azure Document Intelligence.
    
    Args:
        pdf_bytes: PDF file as bytes
        filename: Original filename
        
    Returns:
        Dictionary containing extracted PO data
    """
    extracted_data = {
        "filename": filename,
        "extraction_method": "document_intelligence",
        "fields": {},
        "key_value_pairs": {},
        "tables": []
    }
    
    try:
        logger.info(f"Extracting data from {filename} using Document Intelligence...")
        
        poller = document_client.begin_analyze_document(
            model_id="prebuilt-document",
            document=pdf_bytes
        )
        result = poller.result()
        
        # Extract key-value pairs
        if result.key_value_pairs:
            for kv in result.key_value_pairs:
                if kv.key and kv.value:
                    key_text = kv.key.content if kv.key.content else ""
                    value_text = kv.value.content if kv.value.content else ""
                    extracted_data["key_value_pairs"][key_text] = value_text
            logger.info(f"Extracted {len(extracted_data['key_value_pairs'])} key-value pairs")
        
        # Extract tables
        if result.tables:
            for table_idx, table in enumerate(result.tables):
                table_data = {
                    "row_count": table.row_count,
                    "column_count": table.column_count,
                    "cells": []
                }
                for cell in table.cells:
                    table_data["cells"].append({
                        "row_index": cell.row_index,
                        "column_index": cell.column_index,
                        "content": cell.content
                    })
                extracted_data["tables"].append(table_data)
            logger.info(f"Extracted {len(result.tables)} tables")
        
        # Extract general content
        if result.content:
            extracted_data["full_content"] = result.content
            logger.info(f"Extracted full content ({len(result.content)} characters)")
        
        logger.info(f"✓ Successfully extracted data from {filename}")
        
    except Exception as e:
        logger.error(f"Document Intelligence data extraction failed for {filename}: {e}")
        extracted_data["extraction_method"] = "failed"
        extracted_data["error"] = str(e)
    
    return extracted_data


def save_pdf_to_file(pdf_bytes: bytes, output_path: str) -> bool:
    """
    Save PDF bytes to a file.
    
    Args:
        pdf_bytes: PDF content as bytes
        output_path: Full path where to save the file
        
    Returns:
        True if successful, False otherwise
    """
    try:
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        with open(output_path, 'wb') as f:
            f.write(pdf_bytes)
        logger.info(f"✓ Saved PDF to: {output_path}")
        return True
    except Exception as e:
        logger.error(f"Error saving PDF to {output_path}: {e}")
        return False


def extract_pos_from_pdf(pdf_bytes: bytes, original_filename: str, 
                         output_dir: str = "./output",
                         use_di: bool = True) -> List[PODocumentInfo]:
    """
    Main function to extract and process multiple POs from a single PDF.
    Uses actual PDF page positions to split documents.
    
    Args:
        pdf_bytes: Raw bytes of the input PDF
        original_filename: Original filename for reference
        output_dir: Directory where to save split PO files
        use_di: Whether to use Document Intelligence
        
    Returns:
        List of PODocumentInfo objects for each extracted PO
    """
    logger.info(f"\n{'='*80}")
    logger.info(f"Starting PO extraction from: {original_filename}")
    logger.info(f"File size: {len(pdf_bytes)} bytes")
    logger.info(f"Using Document Intelligence: {use_di}")
    logger.info(f"{'='*80}\n")
    
    # Clear cache at start of new document processing
    di_cache.clear()
    logger.info("Cleared Document Intelligence cache")
    
    # Step 1: Split PDF into individual pages
    logger.info("STEP 1: Splitting PDF into pages...")
    page_bytes_list = split_pdf_pages(pdf_bytes)
    if not page_bytes_list:
        logger.error("Failed to split PDF into pages")
        return []
    
    # Step 2: Detect document boundaries
    logger.info("\nSTEP 2: Detecting document boundaries...")
    boundaries = detect_document_boundaries(page_bytes_list, use_di=use_di)
    
    # Step 3: Group pages by boundaries
    logger.info("\nSTEP 3: Grouping pages by boundaries...")
    groups = group_pages_by_boundaries(page_bytes_list, boundaries)
    
    # Step 4: Process each group
    logger.info("\nSTEP 4: Processing each document group...")
    po_documents = []
    base_filename = os.path.splitext(original_filename)[0]
    
    for idx, group in enumerate(groups, 1):
        if not group['pages']:
            logger.warning(f"Group {idx}: No pages to process, skipping")
            continue
        
        page_range = group['page_range']
        logger.info(f"\n{'─'*60}")
        logger.info(f"Processing Document {idx}/{len(groups)}")
        logger.info(f"Original page range: {page_range}")
        logger.info(f"New page count: {len(group['pages'])} (renumbered 1-{len(group['pages'])})")
        logger.info(f"{'─'*60}")
        
        # Assemble PDF (pages will be automatically renumbered)
        po_pdf_bytes = assemble_pdf_from_pages(group['pages'])
        
        # Try to extract PO number from first page for filename
        po_number = None
        if use_di:
            # Use cached DI result if available
            di_result = di_cache.get(group['pages'][0])
            if di_result:
                po_number = extract_po_number_from_di_result(di_result)
            else:
                # Analyze if not in cache
                di_result = analyze_page_with_di(group['pages'][0])
                if di_result:
                    po_number = extract_po_number_from_di_result(di_result)
        
        if not po_number:
            po_number = extract_po_number_fallback(group['pages'][0])
        
        # Generate filename
        if po_number:
            safe_po = po_number.replace('/', '_').replace(' ', '_')
            po_filename = f"{base_filename}_PO_{safe_po}_pages{page_range.replace('-', '_')}.pdf"
        else:
            po_filename = f"{base_filename}_Document{idx}_pages{page_range.replace('-', '_')}.pdf"
        
        po_filepath = os.path.join(output_dir, po_filename)
        
        # Save to file
        if not save_pdf_to_file(po_pdf_bytes, po_filepath):
            logger.error(f"Failed to save {po_filename}, skipping...")
            continue
        
        # Extract data using Document Intelligence
        extracted_data = extract_po_data_with_di(po_pdf_bytes, po_filename) if use_di else {}
        
        # Extract page number info from first page
        page_number_info = None
        page_info = extract_page_number_from_text(group['pages'][0])
        if not page_info and use_di:
            # Use cached DI result if available
            di_result = di_cache.get(group['pages'][0])
            if di_result:
                page_info = extract_page_number_from_di_result(di_result)
            else:
                # Analyze if not in cache
                di_result = analyze_page_with_di(group['pages'][0])
                if di_result:
                    page_info = extract_page_number_from_di_result(di_result)
        
        if page_info:
            if page_info[1]:  # Has total pages
                page_number_info = f"{page_info[0]}/{page_info[1]}"
            else:
                page_number_info = f"Page {page_info[0]}"
        
        # Create document info
        doc_info = PODocumentInfo(
            filename=po_filename,
            po_number=po_number,
            page_number_info=page_number_info,
            extracted_data=extracted_data,
            page_count=len(group['pages']),
            original_page_range=page_range,
            processed_at=time.time(),
            file_hash=calculate_file_hash(po_pdf_bytes),
            sequence_number=idx,
            grouping_method="boundary_detection"
        )
        
        po_documents.append(doc_info)
        logger.info(f"✓ Successfully processed: {po_filename}")
    
    logger.info(f"\n{'='*80}")
    logger.info(f"Extraction complete!")
    logger.info(f"Total documents extracted: {len(po_documents)}")
    logger.info(f"Total Document Intelligence API calls: {len(di_cache.cache)}")
    logger.info(f"Output directory: {output_dir}")
    logger.info(f"{'='*80}\n")
    
    return po_documents


def main():
    """Example usage of the PO extraction system."""
    
    # Configuration
    input_pdf_path = "sample_multi_po.pdf"  # Change this to your PDF file
    output_directory = "./extracted_pos"
    use_document_intelligence = True  # Set to False to use only regex
    
    try:
        # Read the input PDF
        logger.info(f"Reading input file: {input_pdf_path}")
        with open(input_pdf_path, 'rb') as f:
            pdf_bytes = f.read()
        
        # Extract and process POs
        po_documents = extract_pos_from_pdf(
            pdf_bytes=pdf_bytes,
            original_filename=os.path.basename(input_pdf_path),
            output_dir=output_directory,
            use_di=use_document_intelligence
        )
        
        # Print detailed summary
        print("\n" + "="*80)
        print("EXTRACTION SUMMARY")
        print("="*80)
        
        for doc in po_documents:
            print(f"\n{'─'*80}")
            print(f"Filename: {doc.filename}")
            print(f"Original Pages: {doc.original_page_range} (from source PDF)")
            print(f"New Page Count: {doc.page_count} (renumbered as 1-{doc.page_count})")
            if doc.page_number_info:
                print(f"Page Number on Document: {doc.page_number_info}")
            if doc.po_number:
                print(f"PO Number: {doc.po_number}")
            print(f"File Hash: {doc.file_hash[:32]}...")
            print(f"Processed: {time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(doc.processed_at))}")
            
            if doc.extracted_data and "key_value_pairs" in doc.extracted_data:
                kv_count = len(doc.extracted_data["key_value_pairs"])
                if kv_count > 0:
                    print(f"Key-Value Pairs: {kv_count}")
                    for i, (k, v) in enumerate(list(doc.extracted_data["key_value_pairs"].items())[:3]):
                        print(f"  • {k}: {v}")
            
            if doc.extracted_data and "tables" in doc.extracted_data:
                table_count = len(doc.extracted_data["tables"])
                if table_count > 0:
                    print(f"Tables: {table_count}")
        
        print("\n" + "="*80)
        print(f"Total documents extracted: {len(po_documents)}")
        print(f"Output directory: {output_directory}")
        print("\nHow it works:")
        print("  1. Detects where 'Page 1' appears (text or DI)")
        print("  2. Detects when PO number changes (DI or regex)")
        print("  3. Splits at these boundaries")
        print("  4. Renumbers each document's pages starting from 1")
        print("="*80)
        
    except FileNotFoundError:
        logger.error(f"Input file not found: {input_pdf_path}")
        print(f"\n❌ Error: File '{input_pdf_path}' not found!")
        print("Please update the 'input_pdf_path' variable with your PDF file path.")
    except Exception as e:
        logger.error(f"Error during processing: {e}", exc_info=True)
        print(f"\n❌ Error: {e}")


if __name__ == "__main__":
    main()